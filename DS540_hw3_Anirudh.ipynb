{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Question 6\n",
        "# The ipynb files had to be modified based on the transformers used in custom transformers\n",
        "# We can see that the f1 scores are more or less similar to that of the WEKA results\n",
        "# The same goes for the Logistic Regression model. Although I don't have access to the inner workings of the\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = pd.read_csv('/content/UniversalBank.csv')\n",
        "\n",
        "data.drop(columns=['ID', 'ZIP Code'], errors='ignore', inplace=True)\n",
        "\n",
        "data = pd.get_dummies(data, columns=['Education'], drop_first=True)\n",
        "\n",
        "y = data['Personal Loan']\n",
        "X = data.drop(columns=['Personal Loan'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "param_grid_lr = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "\n",
        "best_lr = grid_search_lr.best_estimator_\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "print(\"Best Logistic Regression Model:\", grid_search_lr.best_params_)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [3, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "\n",
        "grid_search_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "best_dt = grid_search_dt.best_estimator_\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "print(\"Best Decision Tree Model:\", grid_search_dt.best_params_)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "print(\"Best Random Forest Model:\", grid_search_rf.best_params_)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Coui5px-8Rm5",
        "outputId": "83fbdae3-c38c-4b50-f1d9-2b406e13fb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression Model: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Logistic Regression Accuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       895\n",
            "           1       0.91      0.76      0.83       105\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.94      0.88      0.91      1000\n",
            "weighted avg       0.97      0.97      0.97      1000\n",
            "\n",
            "Best Decision Tree Model: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Decision Tree Accuracy: 0.991\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       895\n",
            "           1       0.98      0.93      0.96       105\n",
            "\n",
            "    accuracy                           0.99      1000\n",
            "   macro avg       0.99      0.97      0.98      1000\n",
            "weighted avg       0.99      0.99      0.99      1000\n",
            "\n",
            "Best Random Forest Model: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Random Forest Accuracy: 0.991\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       895\n",
            "           1       0.98      0.93      0.96       105\n",
            "\n",
            "    accuracy                           0.99      1000\n",
            "   macro avg       0.99      0.97      0.98      1000\n",
            "weighted avg       0.99      0.99      0.99      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "DIEmiLRZB8gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7\n",
        "\n",
        "\n",
        "Now the problem with this is that the pipeline was only dealing with one dataset. Here, we already have the split doen for us. This means that we can just use preprocessing as is and get the fields adjusted based on the attributes for the sonar data now."
      ],
      "metadata": {
        "id": "BuJc0q3TCeE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN: Performance depends on the choice of k. If the dataset is noisy or has similar class distributions, KNN may misclassify.\n",
        "\n",
        "Na√Øve Bayes: Works well if features our are independent, but the assumption for the assignment is that it might not hold in Sonar data.\n",
        "\n",
        "SVM: Works well for high-dimensional data like this one but we know that it  requires tuning. If the classes sonar has have non linear relationships and are not linearly separable, kernel selection becomes more important with SVM."
      ],
      "metadata": {
        "id": "nT93ophuEzQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "9NN4Zz-bAVbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Custom transformers was the main issue\n",
        "# you will need to\n",
        "#as the exception, I have used the pipeline from sklearn in case this does not work.\n",
        "try:\n",
        "    from custom_transformers import DataFrameSelector, ZeroVariance, FindCorrelation\n",
        "    from custom_transformers import OptionalStandardScaler, ManualDropper, PipelineChecker\n",
        "except ImportError:\n",
        "    print(\"Custom transformers not found.\")\n",
        "\n",
        "train_data = pd.read_csv(\"/content/sonar_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/sonar_test.csv\")\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "X_test = test_data.iloc[:, :-1]\n",
        "y_test = test_data.iloc[:, -1]\n",
        "base_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "knn_pipeline = Pipeline([\n",
        "    ('preprocess', base_pipeline),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "nb_pipeline = Pipeline([\n",
        "    ('preprocess', base_pipeline),\n",
        "    ('nb', GaussianNB())\n",
        "])\n",
        "\n",
        "svm_pipeline = Pipeline([\n",
        "    ('preprocess', base_pipeline),\n",
        "    ('svm', SVC(kernel='linear', C=1.0))\n",
        "])\n",
        "\n",
        "\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "nb_pipeline.fit(X_train, y_train)\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "knn_pred = knn_pipeline.predict(X_test)\n",
        "nb_pred = nb_pipeline.predict(X_test)\n",
        "svm_pred = svm_pipeline.predict(X_test)\n",
        "knn_acc = accuracy_score(y_test, knn_pred)\n",
        "nb_acc = accuracy_score(y_test, nb_pred)\n",
        "svm_acc = accuracy_score(y_test, svm_pred)\n",
        "\n",
        "print(f\"KNN Accuracy: {knn_acc:.4f}\")\n",
        "print(f\"Naive Bayes Accuracy: {nb_acc:.4f}\")\n",
        "print(f\"SVM Accuracy: {svm_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "V1dYNOoiFSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CDXPAPhUFp_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}